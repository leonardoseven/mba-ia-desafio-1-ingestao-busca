# Desafio MBA Engenharia de Software com IA - Full Cycle

Sistema de busca semÃ¢ntica usando RAG (Retrieval-Augmented Generation) com LangChain, PostgreSQL com pgvector, suportando OpenAI e Google Gemini.

## ğŸ“‹ Requisitos

- Python 3.8+ instalado
- Docker e Docker Compose instalados
- Conta OpenAI ou Google (para API keys)

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clonar o repositÃ³rio (se aplicÃ¡vel)

```bash
git clone <url-do-repositorio>
cd 1-langchain_busca_semantica
```

### 2. Criar ambiente virtual

**Windows:**
```bash
python -m venv venv
cd venv\Scripts
activate
cd ..\..
```

**Linux/Mac:**
```bash
python -m venv venv
source venv/bin/activate
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configurar variÃ¡veis de ambiente

Crie um arquivo `.env` na raiz do projeto com as seguintes variÃ¡veis:

```env
# ConfiguraÃ§Ã£o do Banco de Dados PostgreSQL
DATABASE_URL=postgresql://postgres:postgres@localhost:5439/rag
PG_VECTOR_COLLECTION_NAME=documentos_collection

# Caminho do arquivo PDF para ingestÃ£o
PDF_PATH=./document.pdf

# ConfiguraÃ§Ã£o de Embeddings (escolha um provider)
EMBEDDING_PROVIDER=openai  # ou "gemini"
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
GEMINI_EMBEDDING_MODEL=models/embedding-001

# ConfiguraÃ§Ã£o de LLM (escolha um provider)
LLM_PROVIDER=gemini  # ou "openai"

# API Keys (configure conforme o provider escolhido)
OPENAI_API_KEY=sua_chave_openai_aqui
GOOGLE_API_KEY=sua_chave_google_aqui
```

**Importante:**
- Para usar OpenAI: configure `OPENAI_API_KEY`
- Para usar Gemini: configure `GOOGLE_API_KEY`
- VocÃª pode usar embeddings de um provider e LLM de outro (ex: embeddings OpenAI + LLM Gemini)

### 5. Subir o banco de dados PostgreSQL com pgvector

```bash
docker-compose up -d
```

Este comando irÃ¡:
- Subir um container PostgreSQL na porta 5439
- Criar automaticamente a extensÃ£o `vector` (pgvector)
- Banco de dados: `rag`
- UsuÃ¡rio: `postgres`
- Senha: `postgres`

**Verificar se estÃ¡ rodando:**
```bash
docker ps
```

VocÃª deve ver o container `postgres_rag` rodando.

## ğŸ“„ Processo de IngestÃ£o de Documentos

ApÃ³s configurar o ambiente, Ã© necessÃ¡rio fazer a ingestÃ£o do PDF no banco de dados vetorial.

### Executar ingestÃ£o

```bash
python src/ingest.py
```

Este processo irÃ¡:
1. Validar as variÃ¡veis de ambiente
2. Carregar o PDF do caminho especificado em `PDF_PATH`
3. Dividir o documento em chunks de 1000 caracteres (com overlap de 150)
4. Gerar embeddings para cada chunk usando o provider configurado
5. Armazenar os chunks no PostgreSQL com pgvector

**Tempo estimado:** Depende do tamanho do PDF (alguns segundos a minutos)

## ğŸ’¬ Usar o Chat para Busca SemÃ¢ntica

ApÃ³s a ingestÃ£o, vocÃª pode fazer perguntas sobre o documento usando busca semÃ¢ntica.

### Executar chat

```bash
python src/chat.py
```

Este comando iniciarÃ¡ um chat interativo onde vocÃª pode:
1. Digitar perguntas sobre o conteÃºdo do PDF
2. Receber respostas baseadas apenas no contexto do documento
3. Continuar fazendo perguntas (loop interativo)
4. Para sair, use `Ctrl+C`

**Exemplo de uso:**
```
Digite sua pergunta: O que Ã© inteligÃªncia artificial?
[Resposta baseada no documento...]

Digite sua pergunta: Quais sÃ£o as principais aplicaÃ§Ãµes mencionadas?
[Resposta baseada no documento...]
```

## ğŸ—ï¸ Estrutura do Projeto

```
1-langchain_busca_semantica/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py       # Script de ingestÃ£o de documentos
â”‚   â”œâ”€â”€ search.py       # MÃ³dulo de busca semÃ¢ntica e RAG
â”‚   â””â”€â”€ chat.py         # Interface de chat interativo
â”œâ”€â”€ docker-compose.yml  # ConfiguraÃ§Ã£o do PostgreSQL com pgvector
â”œâ”€â”€ requirements.txt    # DependÃªncias Python
â”œâ”€â”€ .env               # VariÃ¡veis de ambiente (criar manualmente)
â”œâ”€â”€ document.pdf       # Documento para ingestÃ£o
â””â”€â”€ README.md          # Este arquivo
```

## ğŸ”§ Comandos Ãšteis

### Ver logs do PostgreSQL
```bash
docker-compose logs postgres
```

### Parar o banco de dados
```bash
docker-compose down
```

### Parar e remover volumes (limpar dados)
```bash
docker-compose down -v
```

### Reinstalar dependÃªncias
```bash
pip install --force-reinstall -r requirements.txt
```

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Alterar provider de embeddings/LLM


### Alterar modelo usado

Edite no `.env`:
- `OPENAI_EMBEDDING_MODEL=text-embedding-3-small` (para outros embeddings OpenAI)
- `GEMINI_EMBEDDING_MODEL=models/embedding-001` (para outros embeddings Gemini)

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Environment variable X is not set"
- Verifique se o arquivo `.env` existe na raiz do projeto
- Confirme que todas as variÃ¡veis necessÃ¡rias estÃ£o configuradas

### Erro: "PDF file not found"
- Verifique se o caminho em `PDF_PATH` estÃ¡ correto
- Use caminho absoluto ou relativo ao diretÃ³rio raiz

### Erro: "Connection refused" no banco de dados
- Verifique se o Docker estÃ¡ rodando: `docker ps`
- Confirme se o PostgreSQL estÃ¡ ativo: `docker-compose ps`
- Verifique se a porta 5439 estÃ¡ disponÃ­vel

### Erro de API Key invÃ¡lida
- Verifique se as chaves estÃ£o corretas no `.env`
- Para OpenAI: obtenha em https://platform.openai.com/api-keys
- Para Gemini: obtenha em https://makersuite.google.com/app/apikey

## ğŸ“š Tecnologias Utilizadas

- **LangChain**: Framework para construÃ§Ã£o de aplicaÃ§Ãµes LLM
- **PostgreSQL**: Banco de dados relacional
- **pgvector**: ExtensÃ£o para armazenar e buscar vetores
- **OpenAI API**: Para embeddings e modelos GPT
- **Google Gemini API**: Para embeddings e modelos Gemini
- **Python 3.8+**: Linguagem de programaÃ§Ã£o

## ğŸ“ Notas

- O sistema usa RAG (Retrieval-Augmented Generation) para responder perguntas baseadas apenas no contexto do documento
- Se uma pergunta nÃ£o estiver no contexto, o sistema responderÃ¡: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
- VocÃª pode fazer ingestÃ£o mÃºltiplas vezes - novos documentos serÃ£o adicionados Ã  coleÃ§Ã£o existente
- Para limpar o banco e comeÃ§ar do zero, pare o Docker e remova os volumes: `docker-compose down -v`
